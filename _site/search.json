[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Statistical Stock Portfolio Optimization\n\n\nApplying my coursework from STATS C183/C283\n\n\n\nR\n\n\nproject\n\n\nUCLA\n\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\nNaren Prakash\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Airbnb Prices from a Geostatistical Approach\n\n\nApplying my coursework from STATS C173/C273\n\n\n\nR\n\n\nproject\n\n\nUCLA\n\n\n\n\n\n\n\n\n\nApr 6, 2025\n\n\nNaren Prakash\n\n\n\n\n\n\n\n\n\n\n\n\nA new blog home\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nNaren Prakash\n\n\n\n\n\n\n\n\n\n\n\n\nWhy I made this website\n\n\nA 2024 Winter Break project I finished up right before the new year\n\n\n\nnews\n\n\nproject\n\n\n\n\n\n\n\n\n\nDec 31, 2024\n\n\nNaren Prakash\n\n\n\n\n\n\n\n\n\n\n\n\nHow I ended up wanting to become a data scientist (a long post)\n\n\nA decision I made in the summer of 2022\n\n\n\ngoals\n\n\ncareer\n\n\n\n\n\n\n\n\n\nDec 24, 2024\n\n\nNaren Prakash\n\n\n\n\n\n\n\n\n\n\n\n\nAutoencoder for Anomaly Detection\n\n\nDone as a ML / Data Science Intern @ Halliburton\n\n\n\ninternship\n\n\nproject\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 24, 2024\n\n\nNaren Prakash\n\n\n\n\n\n\n\n\n\n\n\n\nData Generator\n\n\nDone as a ML / Data Science Intern @ Halliburton\n\n\n\ninternship\n\n\nproject\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 24, 2024\n\n\nNaren Prakash\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/portfolio-optim/index.html",
    "href": "posts/portfolio-optim/index.html",
    "title": "Statistical Stock Portfolio Optimization",
    "section": "",
    "text": "The lines don’t look like this at all at the time I’m making this post\n\n\nIn this post, I’ll be optimizing a portfolio of selected stocks in different industries with various models as a showcase of what I learned from taking STATS C183/C283 (Statistical Models in Finance) with my GOAT Professor Christou.\nThe portfolio is constructed as follows.\nFive main industries: Technology, Financial Services, Healthcare, Consumer Cyclical, Communication Services\n(Note: All data comes from Yahoo Finance, including historical pricing and specific stock information. This project was done with data from January 2016 to September 2024.)\nTechnology: AAPL (Apple), MSFT (Microsoft), NVDA (NVIDIA), CRM (Salesforce), CSCO (Cisco), ORCL (Oracle)\nFinancial Services: BRK-B (Berkshire Hathaway Inc Class B), JPM (JP Morgan), BAC (Bank of America), WFC (Wells Fargo), BX (Blackstone), GS (Goldman Sachs)\nHealthcare: LLY (Eli Lilly), UNH (UnitedHealth), JNJ (Johnson & Johnson), ABBV (AbbVie), TMO (Thermo Fisher Scientific), AMGN (Amgen)\nConsumer Cyclical: AMZN (Amazon), TSLA (Tesla), HD (Home Depot), MCD (McDonald’s), NKE (Nike), TJX\nCommunication Services: GOOG (Google), META, NFLX (Netflix), VZ (Verizon), DIS (Disney), T (AT&T)\nWe then use the S&P 500 (^GSPC) as our market index for comparison."
  },
  {
    "objectID": "posts/data-generator/index.html",
    "href": "posts/data-generator/index.html",
    "title": "Data Generator",
    "section": "",
    "text": "With such large amounts of data that can’t fit in memory alone, how can we use all of it for training?"
  },
  {
    "objectID": "posts/data-generator/index.html#the-problem",
    "href": "posts/data-generator/index.html#the-problem",
    "title": "Data Generator",
    "section": "The Problem",
    "text": "The Problem\nWhen training a machine learning model, you always want to use as much data as possible, right? But what happens when the data available for training is so large that it won’t fit into memory?\nWith the limitations of machine memory, we often face a challenge: Should we use as much data as we can and stop there?\nThis is where the Data Generator comes into play.\nOur goal is to create a solution that splits large datasets into manageable chunks, allowing us to create and label small batches of data. This approach allows us to feed the model training data one piece at a time, without overwhelming memory.\nAt the same time, we want to track how these data batches are created, so we can ensure accuracy and transparency in the data generation process."
  },
  {
    "objectID": "posts/data-generator/index.html#flowchart",
    "href": "posts/data-generator/index.html#flowchart",
    "title": "Data Generator",
    "section": "Flowchart",
    "text": "Flowchart\n\n\n\nData to model process"
  },
  {
    "objectID": "posts/data-generator/index.html#basic-structure",
    "href": "posts/data-generator/index.html#basic-structure",
    "title": "Data Generator",
    "section": "Basic Structure",
    "text": "Basic Structure\nThe basic framework I used for creating this is from a blog post from Stanford and MIT grad students Afshine Amidi and Shervine Amidi. This framework uses TensorFlow and Keras to split files and create batches of data with a custom DataGenerator class. However, this framework has two main issues:\n\nThe data generator works wonderfully for sizes near that of the testing for this framework (around 65,000 rows), but when scaling to much larger sizes, the performance drops.\nThe framework uses split files with observations but does not show how the data is split. Instead, it shows what is done after the splitting has already been completed."
  },
  {
    "objectID": "posts/data-generator/index.html#using-dask",
    "href": "posts/data-generator/index.html#using-dask",
    "title": "Data Generator",
    "section": "Using Dask",
    "text": "Using Dask\nDask is a Python library for parallel computing that helps scale our Pandas-based code to much larger datasets. It essentially stores Pandas dataframes within Pandas dataframes and offers delayed computation to offset computational problems. We can use this for splitting the files (as Dask allows us to load the entire dataset at once) and loading individual IDs in the DataGenerator class. With this, we can handle amounts of up to 1-10 TB (per Dask’s own testing) for our company data."
  },
  {
    "objectID": "posts/data-generator/index.html#how-do-we-split-the-files",
    "href": "posts/data-generator/index.html#how-do-we-split-the-files",
    "title": "Data Generator",
    "section": "How Do We Split the Files?",
    "text": "How Do We Split the Files?\nWhile the given framework splits files by single observations, this would simply take up too much storage for much larger datasets, such as the one we have (with millions of rows). Instead, we group observations into much smaller groups, keeping them small enough to not surpass our storage capacity. For my working example, I grouped observations based on the month and day of each timestamp (one file per combination)."
  },
  {
    "objectID": "posts/data-generator/index.html#final-product",
    "href": "posts/data-generator/index.html#final-product",
    "title": "Data Generator",
    "section": "Final Product",
    "text": "Final Product\nBased on local testing, our adapted framework was able to handle data more than 10x the size of the original framework! While the upper limit is yet to be found for further testing, the scalability of the tools used to build this generator gives us hope that it can serve as the foundation for a fully functional Data Generator in the future.\nIf you would like to see the code I used for this project or have any questions, feel free to contact me at any of the places listed in my contact section!"
  },
  {
    "objectID": "posts/portfolio-website/index.html",
    "href": "posts/portfolio-website/index.html",
    "title": "Why I made this website",
    "section": "",
    "text": "Note: This is about my professional website. Check it out if you haven’t!"
  },
  {
    "objectID": "posts/portfolio-website/index.html#my-own-little-place-to-post-into-the-void",
    "href": "posts/portfolio-website/index.html#my-own-little-place-to-post-into-the-void",
    "title": "Why I made this website",
    "section": "My own little place to post into the void",
    "text": "My own little place to post into the void\nI can’t tell you how many times I’ve looked up cool people on Google and they’ve had their own little nice website with fancy logos and nice-looking pages and all that. To be frank, that was probably the main reason I wanted my own website. I too wanted a shiny place that was all about me.\nDespite having this in mind for quite a while, I never actually tried to make one of my own until recently. For the longest time, I had assumed even getting a domain name would mean shelling out money for eternity just to have a site I could send to people. Who knew how many people would actually look at it, let alone view the other content?\nUltimately, what pushed me to really try it out was wanting to just have one place where I could show all my friends and family the little things I did in my free time that I thought were cool. At the end of the day, even if no one else viewed this website organically, I would be happy just having a place with a nice little write-up for everything I’ve tinkered with. The sudden motivation for this website really came in late September 2024 on a random weekend. That’s what led me to create my first website using Netlify. I didn’t exactly know how to do web development with code directly, so I just used their visual editor and a website template and called it a day. One night later, I had a fully functional website with posts I could show everyone. However, the longer I had the site, the more discontent I became with it. I almost felt like I cheated to get a website that wasn’t even cohesive and didn’t represent what I wanted my website to say about me. With a busy school quarter, I pushed it down the line with the intention of eventually creating a site I was proud of.\nI hadn’t used JavaScript and CSS in an extremely long time. Not since one of my first projects ever that I created in high school—a basketball player comparison tool that scraped data from the web—had I attempted to make a whole website of my own. Even then, it was barely something that could be called a website. It was a bare-bones page with the project tool as the main focus, and the website was basically just a place to have it. I wanted to use Next.js because I’d joined a club in the fall that created really nice-looking visuals and animations with that as their main language. I had no idea what it was, but with the help of YouTube videos and the internet, I was able to cobble up a functional website I can say I am proud of.\nIs my website amazing and impressive? I don’t know, but I’m glad I have my own little place to put things that I want to see. The experience of creating the site was a blast, and I hope to mess around with a lot more in the future so I can add even more to this website. If anyone stumbles across this in the future and would like to give me feedback on my website (or even my code), I would greatly appreciate it. This is something I hope to continue working on for the rest of my life, and I hope 2025 brings more things for me to be proud of. Here’s to the future.\nThank you for reading my rambling justification for the existence of this website. If you think I could make this better (I definitely can), I would love to hear new ideas.\nYou can check out the code I used to create it here!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "A new blog home",
    "section": "",
    "text": "This is very much not a sponsored post (though if they wanted to sponsor me I would absolutely accept) but after “discovering” quarto, I’ve decided to move my personal blog and my more in-depth project writeups to this quarto based site!\nI first learned about quarto this past quarter, thanks to my wonderful Professor Vivian Lew who I had the pleasure of having this past winter quarter for the first part of my statistical consulting capstone at UCLA. As someone who loves scientific computing, it has completely revolutionized a bunch of the things I did before. I love how markdown based it is, as I switched to markdown for all my note-taking prior to my final year here at UCLA. It makes formatting a breeze for someone aesthetically challenged like me. It also saves me from having to learn more front-end development (React is cool but I really don’t want to learn more than I need to).\nThat, plus the ability to integrate code directly with R and Python (my two favorite languages) made me ditch the manual .jsx files I was using to write up on my professional website and switch to quarto for every need it could possibly satisfy. That and I thought it might be a good idea to keep my personal little parts of the website and my corner of the Internet in its own place.\nAll of this leads me to a happy declaration that this is my official first quarto blog post and hopefully the first of many more! I hope to make some nice walkthroughs of my favorite personal projects and just some more personal posts to put into the abyss.\nNote: If you’re wondering, the posts below this in chronological order are converted from my previous blog setup."
  },
  {
    "objectID": "posts/career-goal/index.html#who-knows-what-they-want-to-major-in-anyway",
    "href": "posts/career-goal/index.html#who-knows-what-they-want-to-major-in-anyway",
    "title": "How I ended up wanting to become a data scientist (a long post)",
    "section": "Who knows what they want to major in anyway?",
    "text": "Who knows what they want to major in anyway?\nTo be honest I never really anticipated finding myself in this position. For pretty much as long as I could remember, I was convinced I was going to become a scientist. Whether it be biology or chemistry, I knew the classes I enjoyed the most every single year were those science classes. Mathematics was the bane of my existence, a topic I had neither the talent nor the patience for. I would never choose a career with that as the focus.\nEven when it came to applying to college, I applied to colleges based on googling “best colleges for science” and then just choosing some that sounded like they had cool programs. Going into college my aspirations had changed a bit, but I was still of the belief I would become a scientist. I entered UCLA in 2021 as a Biology major, which was what I applied for in most of the applications I sent out. The first few quarters were fun, I mostly took the chemistry courses and math courses required for lower division life science requirements. To my own surprise, I really enjoyed the math for life scientists (shoutout Professor Jukka) but I really enjoyed the chemistry classes too. It wasn’t until my spring quarter that I was finally able to take what I came here for, a biology class.\nIt started off fine it wasn’t exactly some kind of dramatic realization I came to immediately. As the quarter progressed though I found myself falling asleep during lecture and just not being able to invest myself into the content. At first, I thought it was a general problem with my concentration and work. At the same time though, I was taking organic chemistry and seemed to have no issues staying alert in that class. As the year ended, I wasn’t sure if I truly wanted to be a biology student for the rest of my 4 years. That being said, I had already signed up for my classes for the next fall.\nAt first I refused to think of even leaving the life sciences department. If I left, what was the point of my entire first year? Did I just waste money and time? I spent my time reading about every offered major on the UCLA course websites, trying to decide what I really wanted to learn about. When I truly thought about it, I realized that maybe my enjoyment for the life sciences math wasn’t some random exception. After all, I had really enjoyed my time taking AP Statistics back in high school. Despite that, I feared not being proficient enough for the math and thought things might just be easier if I stayed doing what I was doing. After all, I couldn’t even enroll in new classes until much later as new student orientation had already begun.\nI talked to many people before ultimately making my decision, but I entered my new path Fall 2022 as I began taking the math prerequisite courses for becoming a stats major. Though I did have to make some adjustments initially, it didn’t take long for me to realize I had made the right decision for myself. Looking back at my own interests in hindsight, it almost seemed obvious the entire time. When I was little, I loved memorizing sports statistics, especially the stats of my favorite athletes. When I first learned programming and computer science in high school, the first independent projects I created were all sports statistics related. I had just attributed this to my love for sports, but perhaps that interest was always there and I just had a mental block.\nSince changing my major, I can confidently say I have no regrets at all. I’ve truly enjoyed every single class I’ve taken since that day. Every day I spend in class or working on something only furthers my belief that I’m in this for life. Getting paid to do the most fun stuff of all time? That career sounds way too good to be true, but I really cannot think of a better way to live. That being said, I don’t regret the path I was on before switching. Sometimes I think about what I could’ve done with that extra time, but there are many experiences I don’t get in that timeline. As for this timeline, I’m just going to put my head down and keep trying to learn about a field that is always changing. Living in the age of information is truly a blessing, and I fully intend to take advantage of it.\nShoutout to the entire UCLA Statistics department for providing such a fun and comprehensive education. Professor Christou you are my GOAT."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#the-problem",
    "href": "posts/anomaly-detection/index.html#the-problem",
    "title": "Autoencoder for Anomaly Detection",
    "section": "The Problem",
    "text": "The Problem\nWe all know equipment maintenance is important for saving money, reducing waste, and preventing accidents as a result of equipment malfunctions.\nIn this case, we want to make sure company trucks function properly by monitoring the input data from the sensors on each truck. How do we set up an alert system so that when the sensor has abnormal readings, we can immediately take a look at the equipment to prevent accidents?\nTraditional statistical methods have many definitions for an outlier we could use to identify those readings. However, with so much sensor data, it might be better to use a larger scale ML model to identify these in real time so prompt action can be taken."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#idea-reconstruction-autoencoder",
    "href": "posts/anomaly-detection/index.html#idea-reconstruction-autoencoder",
    "title": "Autoencoder for Anomaly Detection",
    "section": "Idea: Reconstruction Autoencoder",
    "text": "Idea: Reconstruction Autoencoder\nThis is where the reconstruction autoencoder comes into play.\nWe can use this machine learning model to identify anomalies in real time! Before delving into how we created it, let’s break down what this means."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#what-even-is-an-autoencoder",
    "href": "posts/anomaly-detection/index.html#what-even-is-an-autoencoder",
    "title": "Autoencoder for Anomaly Detection",
    "section": "What even is an autoencoder?",
    "text": "What even is an autoencoder?\nAn autoencoder is a type of neural network that takes in a large amount of high dimensional data and compresses the information into a smaller representation. After this, it expands the compressed representation into a representation of equal size to the original.\nThere are two main parts of the autoencoder:"
  },
  {
    "objectID": "posts/anomaly-detection/index.html#encoder",
    "href": "posts/anomaly-detection/index.html#encoder",
    "title": "Autoencoder for Anomaly Detection",
    "section": "Encoder",
    "text": "Encoder\nThis is where we compress the input data into a smaller representation.\nFor example, say we have a dataset with 500,000 dimensions. We create a representation of the data with only 300 dimensions."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#decoder",
    "href": "posts/anomaly-detection/index.html#decoder",
    "title": "Autoencoder for Anomaly Detection",
    "section": "Decoder",
    "text": "Decoder\nThis is where we take the compressed representation and reconstruct the original data.\nIn this example, we would take the 300-dimension representation and create a dataset with 500,000 dimensions (just like the original set)."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#why-do-we-compress-the-data-just-to-make-one-thats-the-same-size-as-the-original",
    "href": "posts/anomaly-detection/index.html#why-do-we-compress-the-data-just-to-make-one-thats-the-same-size-as-the-original",
    "title": "Autoencoder for Anomaly Detection",
    "section": "Why do we compress the data just to make one that’s the same size as the original?",
    "text": "Why do we compress the data just to make one that’s the same size as the original?\nIt might seem useless to compress the info and then make it just as big. Isn’t the compressed version just a worse version of the original? Compression means some data is lost so it can’t be as good, right?\nThe true reason is that the eventual output isn’t the important part. Instead, the compressed version of the data (also called the bottleneck) is what is important."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#how-does-the-compressed-data-help-us",
    "href": "posts/anomaly-detection/index.html#how-does-the-compressed-data-help-us",
    "title": "Autoencoder for Anomaly Detection",
    "section": "How does the compressed data help us?",
    "text": "How does the compressed data help us?\nIdeally, the compression forces the neural network to preserve as much important information as possible. We make the bottleneck larger again so we can compare the original data against the compressed version. We can then use this for the anomaly detection portion of our task.\nWe plot the reconstructed output and the original output and look for large differences between the two. If there is a big difference and the reconstruction contains the most important info, then we can identify anomalies by looking at where the differences are. This is because if the most important information isn’t enough to estimate a value accurately, then it is likely not a typical value."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#what-neural-network-architecture-do-we-use",
    "href": "posts/anomaly-detection/index.html#what-neural-network-architecture-do-we-use",
    "title": "Autoencoder for Anomaly Detection",
    "section": "What neural network architecture do we use?",
    "text": "What neural network architecture do we use?\nNow that we know what an autoencoder is, we know that it uses a neural network to accomplish its goal. There are many different types of neural networks though, how do we choose the best type?\nThe three main types (ANNs, CNNs, and RNNs) are all commonly used in ML models. One type of RNN, the LSTM, is especially popular for time series forecasting (what we are doing) because it mitigates the vanishing gradient issue of the default RNN while also allowing us to use momentum to preserve short-term and long-term trends in predictions.\nFor our purposes though, with our large amount of data, an LSTM isn’t viable because of speed limitations and computational efficiency reasons."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#introducing-the-transformer",
    "href": "posts/anomaly-detection/index.html#introducing-the-transformer",
    "title": "Autoencoder for Anomaly Detection",
    "section": "Introducing the Transformer",
    "text": "Introducing the Transformer\nIf you’ve been anywhere near the ML community, I’m sure you’ve heard of the transformer. It’s being used everywhere in the Natural Language Processing (NLP) arena, being the basis of state-of-the-art models like ChatGPT and Google’s BERT.\nIts origin comes from the famous “Attention Is All You Need” paper, which revolutionized the world of NLP but now is being used to transform the world of time series forecasting. (If you want to learn more about how the transformer works and the original paper is a bit too abstract, here is an annotated explanation from Harvard NLP that breaks it down further)\n\nThe characteristics of the transformer itself, such as its attention mechanism that we believe would better capture long and short-term dependencies, seem to be more than suitable for our task. With its speed and ability to handle gigantic amounts of data, we decided this was the best basis for creating our time series forecasting ML model."
  },
  {
    "objectID": "posts/anomaly-detection/index.html#final-product",
    "href": "posts/anomaly-detection/index.html#final-product",
    "title": "Autoencoder for Anomaly Detection",
    "section": "Final Product",
    "text": "Final Product\nFrom implementing this transformer-based autoencoder and training it on a large amount of sensor readings, we were able to produce a prediction model that identified outliers in a computationally efficient way. There remained another big question to solve, though.\nEven with the training and validation process, to be entirely sure we identify outliers correctly we need to train on as much data as possible to reduce the likelihood of false positives.\nWith such large amounts of data that can’t fit in memory alone, how can we use that much for training? Think about ChatGPT for example, how do you train on the whole internet if a machine can’t come close to storing all of it?\nThis leads us to the second project, the Data Generator, which aims to resolve this issue!\nIf you would like to see the code I used for this project or have any questions, feel free to contact me at any of the places listed in my contact section!"
  },
  {
    "objectID": "posts/airbnb-geostats/index.html",
    "href": "posts/airbnb-geostats/index.html",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "",
    "text": "A picture of the Airbnb logo I actually like\nPredicting the price of Airbnb listings is a pretty standard project for most people in the stats / data science / ML space. I mean, the data is fairly comprehensive and large and is easily publicly accessible. So what makes this project any different?\nI wanted to try looking at this pretty standard problem through a completely different lens. After taking STATS C173/273 at UCLA (one class btw it’s just cross-listed so the designation is odd) I learned about kriging and spatial prediction and testing techniques. Specifically, accounting for spatial autocorrelation in a variable for predicting new values of the same value.\nGenerally, this is applied to more natural phenomena, like weather events, that are easily designated as geostatistical events. However, I thought it would make sense to consider Airbnb listing themselves a form of geostatistical data. After all, prices vary by neighborhood and location is a prime determinant of listing price. With that in mind, I decided to see for myself if this was a valid way of price prediction.\nNote: All the data used in this report is from the 14 December 2024 report of Airbnb data from https://insideairbnb.com/get-the-data/.\nBefore I began my actual code, I wanted to set up my R environment to make it a bit more efficient.\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nncores &lt;- detectCores()\nclus &lt;- makeCluster(ncores - 1)\nregisterDoParallel(clus)\nI then loaded in the libraries I planned to use for this project.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::accumulate() masks foreach::accumulate()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n✖ purrr::when()       masks foreach::when()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(geoR)\n\n--------------------------------------------------------------\n Analysis of Geostatistical Data\n For an Introduction to geoR go to http://www.leg.ufpr.br/geoR\n geoR version 1.9-4 (built on 2024-02-14) is now loaded\n--------------------------------------------------------------\n\nlibrary(gstat)\nlibrary(sf)\n\nLinking to GEOS 3.12.2, GDAL 3.9.3, PROJ 9.4.1; sf_use_s2() is TRUE\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\nBefore I started working with the data set, I had to do some modification. I selected the predictors I wanted to consider and made sure that no two listings had the same combination of latitude and longitude. I then dropped any observations with NA values and randomly sampled 1000 observations to create the final data set. After this, I created factors for the appropriate values and fixed some issues via manual inspection.\nset.seed(0)\n\naustin_listings &lt;- austin_listings_orig %&gt;%\n  select(c(longitude, latitude, property_type, room_type, bathrooms, bedrooms, beds, price)) %&gt;%\n  distinct(longitude, latitude, .keep_all = TRUE) %&gt;%\n  drop_na() %&gt;%\n  sample_n(1000) %&gt;%\n  mutate_if(is.character, as.factor)\naustin_listings$price &lt;- as.numeric(gsub(\"[^0-9\\\\.]\", \"\", austin_listings$price))"
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#data-overview",
    "href": "posts/airbnb-geostats/index.html#data-overview",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Data overview",
    "text": "Data overview\n\nlibrary(DataExplorer)\nplot_intro(austin_listings)\n\n\n\n\n\n\n\nkable(summary(austin_listings))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nproperty_type\nroom_type\nbathrooms\nbedrooms\nbeds\nprice\n\n\n\n\n\nMin. :-98.01\nMin. :30.12\nEntire home :430\nEntire home/apt:861\nMin. :0.000\nMin. :0.000\nMin. : 0.000\nMin. : 18.0\n\n\n\n1st Qu.:-97.77\n1st Qu.:30.24\nEntire rental unit :185\nHotel room : 1\n1st Qu.:1.000\n1st Qu.:1.000\n1st Qu.: 1.000\n1st Qu.: 84.0\n\n\n\nMedian :-97.74\nMedian :30.27\nPrivate room in home: 94\nPrivate room :135\nMedian :1.500\nMedian :2.000\nMedian : 2.000\nMedian : 133.0\n\n\n\nMean :-97.75\nMean :30.28\nEntire condo : 73\nShared room : 3\nMean :1.698\nMean :2.079\nMean : 2.897\nMean : 256.9\n\n\n\n3rd Qu.:-97.72\n3rd Qu.:30.31\nEntire guesthouse : 62\nNA\n3rd Qu.:2.000\n3rd Qu.:3.000\n3rd Qu.: 4.000\n3rd Qu.: 225.0\n\n\n\nMax. :-97.56\nMax. :30.51\nEntire guest suite : 22\nNA\nMax. :7.000\nMax. :8.000\nMax. :20.000\nMax. :10000.0\n\n\n\nNA\nNA\n(Other) :134\nNA\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#numerical-exploration",
    "href": "posts/airbnb-geostats/index.html#numerical-exploration",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Numerical exploration",
    "text": "Numerical exploration\n\nlong_hist &lt;- austin_listings %&gt;% ggplot(aes(x = longitude)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nlat_hist &lt;- austin_listings %&gt;% ggplot(aes(x = latitude)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nbed_hist &lt;- austin_listings %&gt;% ggplot(aes(x = bedrooms)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nbath_hist &lt;- austin_listings %&gt;% ggplot(aes(x = bathrooms)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nbeds_hist &lt;- austin_listings %&gt;% ggplot(aes(x = beds)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nprice_hist &lt;- austin_listings %&gt;% ggplot(aes(x = price)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\n\ngrid.arrange(long_hist, lat_hist, bed_hist, bath_hist, beds_hist, price_hist, nrow = 3)\n\n\n\n\n\n\n\n\nWe can see that most of the numeric variables have very skewed distributions."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#categorical-data-exploration",
    "href": "posts/airbnb-geostats/index.html#categorical-data-exploration",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Categorical data exploration",
    "text": "Categorical data exploration\n\nbar_prop &lt;- austin_listings %&gt;% ggplot(aes(x = property_type)) +\n  geom_bar() +\n  coord_flip()\nbar_room &lt;- austin_listings %&gt;% ggplot(aes(x = room_type)) +\n  geom_bar() +\n  coord_flip()\ngrid.arrange(bar_prop, bar_room)\n\n\n\n\n\n\n\n\nThe property type graph shows a clear majority of listings are entire homes or rental units, which is unsurprising given Airbnb marketing. A notable thing I noticed here was that there were listings that were rooms in hotels, which seemed contradictory to the whole concept of Airbnb."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#variable-relationship-exploration",
    "href": "posts/airbnb-geostats/index.html#variable-relationship-exploration",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Variable relationship exploration",
    "text": "Variable relationship exploration\n\nplot_correlation(austin_listings)\n\n1 features with more than 20 categories ignored!\nproperty_type: 36 categories\n\n\n\n\n\n\n\n\n\nWe can see some noticeable correlation among many of the predictors, with the private room type and the entire home apartment room type having the strongest correlation.\nNow that we’ve gotten an understanding of what our data looks like, we proceed to the non-spatial analysis component."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#non-spatial-analysis---lasso-regression-for-variable-selection",
    "href": "posts/airbnb-geostats/index.html#non-spatial-analysis---lasso-regression-for-variable-selection",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Non-spatial analysis - LASSO regression for variable selection",
    "text": "Non-spatial analysis - LASSO regression for variable selection\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-8\n\nset.seed(0)\n\npred &lt;- austin_listings %&gt;% select(-price)\nX &lt;- data.matrix(pred)\ny &lt;- austin_listings$price\n\ncv.model &lt;- cv.glmnet(X, y, alpha = 1)\nlam &lt;- cv.model$lambda.min\n\nbest_mod &lt;- glmnet(X, y, alpha = 1, lambda = lam)\ncoef(best_mod)\n\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n(Intercept)    46.0619\nlongitude       .     \nlatitude        .     \nproperty_type   .     \nroom_type       .     \nbathrooms     124.2221\nbedrooms        .     \nbeds            .     \n\n\nBased on the LASSO method of variable selection, only the variable bathrooms is a significant predictor for the price of a individual listing. However, knowing there is another variable with relation to price helps us decide the specific kriging method for the best price prediction model.\nWith that, we move on to geospatial analysis."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#h-scatterplots-and-correlogram",
    "href": "posts/airbnb-geostats/index.html#h-scatterplots-and-correlogram",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "h-scatterplots and correlogram",
    "text": "h-scatterplots and correlogram\n\nlibrary(sp)\nlibrary(gstat)\n\nsp_listings &lt;- austin_listings\n\ncoordinates(sp_listings) &lt;- ~ longitude + latitude\n\nqq &lt;- hscat(price ~ 1, sp_listings, c(0, 20, 40, 60, 80, 100, 120, 140, 160, 180))\n\nplot(qq, main = \"h-scatterplots\")\n\n\n\n\n\n\n\nplot(c(10, 30, 50, 70, 90, 110, 130, 150, 170), c(0.824, 0.811, 0.734, 0.684, 0.629, 0.517, 0.473, 0.37, 0.264), type = \"l\", xlab = \"Separation distance (h)\", ylab = \"Correlation coefficient (r)\", main = \"Correlogram for AirBnB pricing data\", xaxt = \"n\", yaxt = \"n\")\n\naxis(1, at = seq(10, 240, by = 20), labels = seq(10, 240, by = 20))\n\naxis(2, at = seq(0, 1, by = 0.1), labels = seq(0, 1, by = 0.1))\n\n\n\n\n\n\n\n\nBoth the h-scatterplots and the correlogram indicate that there seems to be spatial correlation between points. This is especially noticeable in the correlogram as we can visualize a clear negative trend in correlation coefficient as the separation distance increases. This seems to justify our spatial correlation approach."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#variograms",
    "href": "posts/airbnb-geostats/index.html#variograms",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Variograms",
    "text": "Variograms\n\nSample variogram\nScaling is done here to make variogram creation easier later on.\n\ng &lt;- gstat(id = \"scaled price\", formula = scale(price) ~ 1, locations = ~ longitude + latitude, data = austin_listings)\nsamp &lt;- variogram(g)\nplot(samp)\n\n\n\n\n\n\n\n\n\n\nDifferent covariance function fits (using Cressie’s weights)\nWe use Cressie’s weights as a compromise between the techniques of ordinary least squares and generalized least squares. This way we can reduce computation but also account for covariance structures.\nThe two selected covariance functions are the exponential and spherical covariance functions.\n\nexp_fit &lt;- fit.variogram(samp, vgm(1.2, \"Exp\", 0.7, 0.3), fit.method = 2)\nsph_fit &lt;- fit.variogram(samp, vgm(1.2, \"Sph\", 0.7, 0.3), fit.method = 2)\n\n\nplot(samp, exp_fit)\n\n\n\n\n\n\n\nplot(samp, sph_fit)\n\n\n\n\n\n\n\n\nVisually it is hard to choose the best covariance function. Instead, we will minimize PRESS (predicted residual error sum of squares). Through cross-validation, we’ll select a covariance function for our model.\n\n\nCross validation\n\nsl &lt;- sample(1:1000, 700)\n\ntrain &lt;- austin_listings[sl, ]\ntest &lt;- austin_listings[-sl, ]\n\n\ncvalid &lt;- krige(id = \"scaledprice\", scale(price) ~ 1, locations = ~ longitude + latitude, model = exp_fit, data = train, newdata = test)\n\n[using ordinary kriging]\n\ndifference &lt;- scale(test$price) - cvalid$scaledprice.pred\nsummary(difference)\n\n       V1          \n Min.   :-2.88004  \n 1st Qu.:-0.20605  \n Median :-0.10193  \n Mean   : 0.01401  \n 3rd Qu.: 0.03235  \n Max.   :13.60095  \n\npress1 &lt;- sum(difference^2)\npress1\n\n[1] 314.8996\n\n\n\ncvalid &lt;- krige(id = \"scaledprice\", scale(price) ~ 1, locations = ~ longitude + latitude, model = sph_fit, data = train, newdata = test)\n\n[using ordinary kriging]\n\ndifference &lt;- scale(test$price) - cvalid$scaledprice.pred\nsummary(difference)\n\n       V1           \n Min.   :-1.941282  \n 1st Qu.:-0.220706  \n Median :-0.125659  \n Mean   : 0.005639  \n 3rd Qu.: 0.000283  \n Max.   :13.555199  \n\npress2 &lt;- sum(difference^2)\npress2\n\n[1] 304.6792\n\n\nWe see here that the spherical covariance model minimizes PRESS so we continue onward with this covariance function."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#kriging",
    "href": "posts/airbnb-geostats/index.html#kriging",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Kriging",
    "text": "Kriging\nWe know there is one related variable, bathrooms, that has the most correlation with price. Let’s investigate whether cokriging with this predictor would be the best approach to price prediction.\n\nOrdinary kriging\n\nx.range &lt;- as.integer(range(austin_listings[, 1]))\ny.range &lt;- as.integer(range(austin_listings[, 2]))\ngrd &lt;- expand.grid(\n  longitude = seq(from = x.range[1], to = x.range[2], by = 0.001),\n  latitude = seq(from = y.range[1], to = y.range[2], by = 0.001)\n)\n\n\npred &lt;- krige.cv(formula = scale(price) ~ 1, data = austin_listings, locations = ~ longitude + latitude)\n\n\nmean(pred$residual)\n\n[1] 0.01774772\n\n\n\n\nOrdinary Co-kriging\nAdding the covariate and looking at the variogram plots:\n\nall &lt;- gstat(g, id = \"bathrooms\", formula = scale(bathrooms) ~ 1, locations = ~ longitude + latitude, data = austin_listings)\n\nvar_all &lt;- variogram(all)\nall_fit &lt;- fit.lmc(var_all, all, model = sph_fit)\nplot(var_all, all_fit)\n\n\n\n\n\n\n\n\n\nck &lt;- predict(all_fit, grd)\n\nLinear Model of Coregionalization found. Good.\n[using ordinary cokriging]\n\n\n\ncok &lt;- gstat.cv(all_fit)\n\nWarning in checkNames(value): attempt to set invalid names: this may lead to\nproblems later on. See ?make.names\n\n\n\nprint(head(pred))\n\n     var1.pred var1.var   observed    residual zscore fold longitude latitude\n1  0.157460703       NA -0.1568927 -0.31435337     NA    1 -97.74668 30.27257\n2  0.004633452       NA  0.2780841  0.27345065     NA    2 -97.72470 30.25387\n3 -0.010727639       NA -0.2721183 -0.26139066     NA    3 -97.72851 30.24386\n4 -0.105715907       NA -0.1266459 -0.02093003     NA    4 -97.74562 30.25795\n5 -0.053681854       NA -0.1684152 -0.11473337     NA    5 -97.74712 30.27016\n6  0.015238444       NA  0.2838454  0.26860694     NA    6 -97.72518 30.25520\n\nprint(head(ck))\n\n  longitude latitude scaled price.pred scaled price.var bathrooms.pred\n1   -98.000       30        0.03859578         1.242966       0.086515\n2   -97.999       30        0.03859578         1.242966       0.086515\n3   -97.998       30        0.03859578         1.242966       0.086515\n4   -97.997       30        0.03859578         1.242966       0.086515\n5   -97.996       30        0.03859578         1.242966       0.086515\n6   -97.995       30        0.03859578         1.242966       0.086515\n  bathrooms.var cov.scaled price.bathrooms\n1      1.073499                  0.2039032\n2      1.073499                  0.2039032\n3      1.073499                  0.2039032\n4      1.073499                  0.2039032\n5      1.073499                  0.2039032\n6      1.073499                  0.2039032\n\n\n\nlibrary(lattice)\n\nlevelplot(ck$`scaled price.pred` ~ longitude + latitude, ck,\n  aspect = \"iso\",\n  main = \"ordinary co-kriging predictions\"\n)\n\n\n\n\n\n\n\nlevelplot(ck$`scaled price.var` ~ longitude + latitude, ck,\n  aspect = \"iso\",\n  main = \"ordinary co-kriging variance\"\n)\n\n\n\n\n\n\n\n\nWe can see lots of uniformity between the co-kriging predictions and variance. Is it better than ordinary kriging?\nIn this case, it looks like ordinary kriging is better as the cokriging results are too uniform and don’t accurately capture the variation in pricing amongst Austin listings. Therefore, our final model is the spherical covariance model with ordinary kriging."
  },
  {
    "objectID": "posts/airbnb-geostats/index.html#note-for-ultimate-kriging-predictions",
    "href": "posts/airbnb-geostats/index.html#note-for-ultimate-kriging-predictions",
    "title": "Analyzing Airbnb Prices from a Geostatistical Approach",
    "section": "Note for ultimate Kriging Predictions",
    "text": "Note for ultimate Kriging Predictions\nIt should be noted that for the purposes of actual price prediction, the inputs of this model must be scaled which means that the resulting predictions have to be un-scaled to the original units."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! My name is Naren Prakash and I’m an upcoming graduate of UCLA majoring in Statistics and Data Science. I really want to become a data scientist and so this is a little chronicle of my personal thoughts and also a bit of my road to hopefully achieving my dream.\nThis is my personal website to post into the abyss about the things I like and things I am doing / want to do in my free time."
  }
]