{
  "hash": "13b4762c45f340e96138ea886b4c361c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Analyzing Airbnb Prices from a Geostatistical Approach\"\nsubtitle: \"Applying my coursework from STATS C173/C283\"\nauthor: \"Naren Prakash\"\ndate: now\ncategories: [R, project, UCLA]\nformat:\n  html:\n    toc: true\n---\n\n\n\n![A picture of the Airbnb logo I actually like](airbnb_logo_detail.jpg)\n\nPredicting the price of Airbnb listings is a pretty standard project for most people in the stats / data science / ML space. I mean, the data is fairly comprehensive and large and is easily publicly accessible. So what makes this project any different? \n\nI wanted to try looking at this pretty standard problem through a completely different lens. After taking STATS C173/283 at UCLA (one class btw it's just cross-listed so the designation is odd) I learned about kriging and spatial prediction and testing techniques. Specifically, accounting for spatial autocorrelation in a variable for predicting new values of the same value. \n\nGenerally, this is applied to more natural phenomena, like weather events, that are easily designated as geostatistical events. However, I thought it would make sense to consider Airbnb listing themselves a form of geostatistical data. After all, prices vary by neighborhood and location is a prime determinant of listing price. With that in mind, I decided to see for myself if this was a valid way of price prediction.\n\n**Note: All the data used in this report is from the 14 December 2024 report of Airbnb data from <https://insideairbnb.com/get-the-data/>.**\n\nBefore I began my actual code, I wanted to set up my R environment to make it a bit more efficient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doParallel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: foreach\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: iterators\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: parallel\n```\n\n\n:::\n\n```{.r .cell-code}\nncores <- detectCores()\nclus <- makeCluster(ncores - 1)\nregisterDoParallel(clus)\n```\n:::\n\n\n\nI then loaded in the libraries I planned to use for this project.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::accumulate() masks foreach::accumulate()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\n✖ purrr::when()       masks foreach::when()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(maps)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(geoR)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n--------------------------------------------------------------\n Analysis of Geostatistical Data\n For an Introduction to geoR go to http://www.leg.ufpr.br/geoR\n geoR version 1.9-4 (built on 2024-02-14) is now loaded\n--------------------------------------------------------------\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(gstat)\nlibrary(sf)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLinking to GEOS 3.12.2, GDAL 3.9.3, PROJ 9.4.1; sf_use_s2() is TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(gridExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n:::\n\n\n\nBefore I started working with the data set, I had to do some modification. I selected the predictors I wanted to consider and made sure that no two listings had the same combination of latitude and longitude. I then dropped any observations with NA values and randomly sampled 1000 observations to create the final data set. After this, I created factors for the appropriate values and fixed some issues via manual inspection.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\n\naustin_listings <- austin_listings_orig %>%\n  select(c(longitude, latitude, property_type, room_type, bathrooms, bedrooms, beds, price)) %>%\n  distinct(longitude, latitude, .keep_all = TRUE) %>%\n  drop_na() %>%\n  sample_n(1000) %>%\n  mutate_if(is.character, as.factor)\naustin_listings$price <- as.numeric(gsub(\"[^0-9\\\\.]\", \"\", austin_listings$price))\n```\n:::\n\n\n# Data Exploration and non-spatial analysis\n\nBefore delving into the analysis, I wanted to do some exploratory data analysis of our finalized dataset.\n\n## Data overview\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DataExplorer)\nplot_intro(austin_listings)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nkable(summary(austin_listings))\n```\n\n::: {.cell-output-display}\n\n\n|   |  longitude    |   latitude   |             property_type |          room_type |  bathrooms   |   bedrooms   |     beds      |    price       |\n|:--|:--------------|:-------------|:--------------------------|:-------------------|:-------------|:-------------|:--------------|:---------------|\n|   |Min.   :-98.01 |Min.   :30.12 |Entire home         :430   |Entire home/apt:861 |Min.   :0.000 |Min.   :0.000 |Min.   : 0.000 |Min.   :   18.0 |\n|   |1st Qu.:-97.77 |1st Qu.:30.24 |Entire rental unit  :185   |Hotel room     :  1 |1st Qu.:1.000 |1st Qu.:1.000 |1st Qu.: 1.000 |1st Qu.:   84.0 |\n|   |Median :-97.74 |Median :30.27 |Private room in home: 94   |Private room   :135 |Median :1.500 |Median :2.000 |Median : 2.000 |Median :  133.0 |\n|   |Mean   :-97.75 |Mean   :30.28 |Entire condo        : 73   |Shared room    :  3 |Mean   :1.698 |Mean   :2.079 |Mean   : 2.897 |Mean   :  256.9 |\n|   |3rd Qu.:-97.72 |3rd Qu.:30.31 |Entire guesthouse   : 62   |NA                  |3rd Qu.:2.000 |3rd Qu.:3.000 |3rd Qu.: 4.000 |3rd Qu.:  225.0 |\n|   |Max.   :-97.56 |Max.   :30.51 |Entire guest suite  : 22   |NA                  |Max.   :7.000 |Max.   :8.000 |Max.   :20.000 |Max.   :10000.0 |\n|   |NA             |NA            |(Other)             :134   |NA                  |NA            |NA            |NA             |NA              |\n\n\n:::\n:::\n\n\n## Numerical exploration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong_hist <- austin_listings %>% ggplot(aes(x = longitude)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nlat_hist <- austin_listings %>% ggplot(aes(x = latitude)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nbed_hist <- austin_listings %>% ggplot(aes(x = bedrooms)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nbath_hist <- austin_listings %>% ggplot(aes(x = bathrooms)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nbeds_hist <- austin_listings %>% ggplot(aes(x = beds)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\nprice_hist <- austin_listings %>% ggplot(aes(x = price)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density(col = \"red\")\n\ngrid.arrange(long_hist, lat_hist, bed_hist, bath_hist, beds_hist, price_hist, nrow = 3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\nWe can see that most of the numeric variables have very skewed distributions.\n\n## Categorical data exploration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbar_prop <- austin_listings %>% ggplot(aes(x = property_type)) +\n  geom_bar() +\n  coord_flip()\nbar_room <- austin_listings %>% ggplot(aes(x = room_type)) +\n  geom_bar() +\n  coord_flip()\ngrid.arrange(bar_prop, bar_room)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThe property type graph shows a clear majority of listings are entire homes or rental units, which is unsurprising given Airbnb marketing. A notable thing I noticed here was that there were listings that were rooms in hotels, which seemed contradictory to the whole concept of Airbnb. \n\n## Variable relationship exploration\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_correlation(austin_listings)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n1 features with more than 20 categories ignored!\nproperty_type: 36 categories\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=1440}\n:::\n:::\n\n\nWe can see some noticeable correlation among many of the predictors, with the private room type and the entire home apartment room type having the strongest correlation.\n\nNow that we've gotten an understanding of what our data looks like, we proceed to the non-spatial analysis component. \n\n## Non-spatial analysis - LASSO regression for variable selection\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(0)\n\npred <- austin_listings %>% select(-price)\nX <- data.matrix(pred)\ny <- austin_listings$price\n\ncv.model <- cv.glmnet(X, y, alpha = 1)\nlam <- cv.model$lambda.min\n\nbest_mod <- glmnet(X, y, alpha = 1, lambda = lam)\ncoef(best_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n(Intercept)    46.0619\nlongitude       .     \nlatitude        .     \nproperty_type   .     \nroom_type       .     \nbathrooms     124.2221\nbedrooms        .     \nbeds            .     \n```\n\n\n:::\n:::\n\n\nBased on the LASSO method of variable selection, only the variable bathrooms is a significant predictor for the price of a individual listing. However, knowing there is another variable with relation to price helps us decide the specific kriging method for the best price prediction model. \n\nWith that, we move on to geospatial analysis.\n\n# Geospatial Analysis\n\n## h-scatterplots and correlogram\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sp)\nlibrary(gstat)\n\nsp_listings <- austin_listings\n\ncoordinates(sp_listings) <- ~ longitude + latitude\n\nqq <- hscat(price ~ 1, sp_listings, c(0, 20, 40, 60, 80, 100, 120, 140, 160, 180))\n\nplot(qq, main = \"h-scatterplots\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(c(10, 30, 50, 70, 90, 110, 130, 150, 170), c(0.824, 0.811, 0.734, 0.684, 0.629, 0.517, 0.473, 0.37, 0.264), type = \"l\", xlab = \"Separation distance (h)\", ylab = \"Correlation coefficient (r)\", main = \"Correlogram for AirBnB pricing data\", xaxt = \"n\", yaxt = \"n\")\n\naxis(1, at = seq(10, 240, by = 20), labels = seq(10, 240, by = 20))\n\naxis(2, at = seq(0, 1, by = 0.1), labels = seq(0, 1, by = 0.1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n\nBoth the h-scatterplots and the correlogram indicate that there seems to be spatial correlation between points. This is especially noticeable in the correlogram as we can visualize a clear negative trend in correlation coefficient as the separation distance increases. This seems to justify our spatial correlation approach.\n\n## Variograms\n\n### Sample variogram\n\nScaling is done here to make variogram creation easier later on.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- gstat(id = \"scaled price\", formula = scale(price) ~ 1, locations = ~ longitude + latitude, data = austin_listings)\nsamp <- variogram(g)\nplot(samp)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Different covariance function fits (using Cressie's weights)\n\nWe use Cressie's weights as a compromise between the techniques of ordinary least squares and generalized least squares. This way we can reduce computation but also account for covariance structures.\n\nThe two selected covariance functions are the exponential and spherical covariance functions,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp_fit <- fit.variogram(samp, vgm(1.2, \"Exp\", 0.7, 0.3), fit.method = 2)\nsph_fit <- fit.variogram(samp, vgm(1.2, \"Sph\", 0.7, 0.3), fit.method = 2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(samp, exp_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(samp, sph_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n:::\n\n\nVisually it is hard to choose the best covariance function. Instead, we will minimize PRESS (predicted residual error sum of squares). Through cross-validation, we'll select a covariance function for our model.\n\n### Cross validation\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsl <- sample(1:1000, 700)\n\ntrain <- austin_listings[sl, ]\ntest <- austin_listings[-sl, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncvalid <- krige(id = \"scaledprice\", scale(price) ~ 1, locations = ~ longitude + latitude, model = exp_fit, data = train, newdata = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[using ordinary kriging]\n```\n\n\n:::\n\n```{.r .cell-code}\ndifference <- scale(test$price) - cvalid$scaledprice.pred\nsummary(difference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       V1          \n Min.   :-2.88004  \n 1st Qu.:-0.20605  \n Median :-0.10193  \n Mean   : 0.01401  \n 3rd Qu.: 0.03235  \n Max.   :13.60095  \n```\n\n\n:::\n\n```{.r .cell-code}\npress1 <- sum(difference^2)\npress1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 314.8996\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncvalid <- krige(id = \"scaledprice\", scale(price) ~ 1, locations = ~ longitude + latitude, model = sph_fit, data = train, newdata = test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[using ordinary kriging]\n```\n\n\n:::\n\n```{.r .cell-code}\ndifference <- scale(test$price) - cvalid$scaledprice.pred\nsummary(difference)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       V1           \n Min.   :-1.941282  \n 1st Qu.:-0.220706  \n Median :-0.125659  \n Mean   : 0.005639  \n 3rd Qu.: 0.000283  \n Max.   :13.555199  \n```\n\n\n:::\n\n```{.r .cell-code}\npress2 <- sum(difference^2)\npress2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 304.6792\n```\n\n\n:::\n:::\n\n\n\nWe see here that the spherical covariance model minimizes PRESS so we continue onward with this covariance function.\n\n## Kriging\n\nWe know there is one related variable, bathrooms, that has the most correlation with price. Let's investigate whether cokriging with this predictor would be the best approach to price prediction.\n\n### Ordinary kriging\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx.range <- as.integer(range(austin_listings[, 1]))\ny.range <- as.integer(range(austin_listings[, 2]))\ngrd <- expand.grid(\n  longitude = seq(from = x.range[1], to = x.range[2], by = 0.001),\n  latitude = seq(from = y.range[1], to = y.range[2], by = 0.001)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- krige.cv(formula = scale(price) ~ 1, data = austin_listings, locations = ~ longitude + latitude)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(pred$residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01774772\n```\n\n\n:::\n:::\n\n\n\n### Ordinary Co-kriging\n\nAdding the covariate and looking at the variogram plots:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall <- gstat(g, id = \"bathrooms\", formula = scale(bathrooms) ~ 1, locations = ~ longitude + latitude, data = austin_listings)\n\nvar_all <- variogram(all)\nall_fit <- fit.lmc(var_all, all, model = sph_fit)\nplot(var_all, all_fit)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nck <- predict(all_fit, grd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Model of Coregionalization found. Good.\n[using ordinary cokriging]\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncok <- gstat.cv(all_fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkNames(value): attempt to set invalid names: this may lead to\nproblems later on. See ?make.names\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(head(pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     var1.pred var1.var   observed    residual zscore fold longitude latitude\n1  0.157460703       NA -0.1568927 -0.31435337     NA    1 -97.74668 30.27257\n2  0.004633452       NA  0.2780841  0.27345065     NA    2 -97.72470 30.25387\n3 -0.010727639       NA -0.2721183 -0.26139066     NA    3 -97.72851 30.24386\n4 -0.105715907       NA -0.1266459 -0.02093003     NA    4 -97.74562 30.25795\n5 -0.053681854       NA -0.1684152 -0.11473337     NA    5 -97.74712 30.27016\n6  0.015238444       NA  0.2838454  0.26860694     NA    6 -97.72518 30.25520\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(head(ck))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  longitude latitude scaled price.pred scaled price.var bathrooms.pred\n1   -98.000       30        0.03859578         1.242966       0.086515\n2   -97.999       30        0.03859578         1.242966       0.086515\n3   -97.998       30        0.03859578         1.242966       0.086515\n4   -97.997       30        0.03859578         1.242966       0.086515\n5   -97.996       30        0.03859578         1.242966       0.086515\n6   -97.995       30        0.03859578         1.242966       0.086515\n  bathrooms.var cov.scaled price.bathrooms\n1      1.073499                  0.2039032\n2      1.073499                  0.2039032\n3      1.073499                  0.2039032\n4      1.073499                  0.2039032\n5      1.073499                  0.2039032\n6      1.073499                  0.2039032\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lattice)\n\nlevelplot(ck$`scaled price.pred` ~ longitude + latitude, ck,\n  aspect = \"iso\",\n  main = \"ordinary co-kriging predictions\"\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlevelplot(ck$`scaled price.var` ~ longitude + latitude, ck,\n  aspect = \"iso\",\n  main = \"ordinary co-kriging variance\"\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-2.png){width=672}\n:::\n:::\n\n\nWe can see lots of uniformity between the co-kriging predictions and variance. **Is it better than ordinary kriging?**\n\nIn this case, it looks like ordinary kriging is better as the cokriging results are too uniform and don't accurately capture the variation in pricing amongst Austin listings. Therefore, our final model is the spherical covariance model with ordinary kriging.\n\n## Ultimate Kriging Predictions\n\nIt should be noted that for the purposes of actual price prediction, the inputs of this model must be scaled which means that the resulting predictions have to be un-scaled to the original units.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}